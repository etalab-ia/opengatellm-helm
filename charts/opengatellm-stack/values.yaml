global:
  storage:
    storageClassName: sbs-default

opengatellm:
  enabled: true

  postgresql:
    cluster:
      initdb:
        postInitApplicationSQL:
          - CREATE DATABASE playground WITH ENCODING 'UTF8';

  opengatellm:
    replicas: 1
    image:
      repository: ghcr.io/etalab-ia/opengatellm/api
      tag: 0.3.6
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 8000

    gunicornArgs: "--workers 4 --worker-connections 1000 --timeout 120 --keep-alive 75 --graceful-timeout 75 --log-config /home/opengatellm/logging/logging.conf"

    logging:
      level: "INFO"

    # ----------------------------------- models ------------------------------------
    models:
      - name: albert-testbed
        type: text-generation
        providers:
          - type: vllm
            model_name: "gemma3:1b"
            url: "http://albert-testbed.etalab.gouv.fr:8000"
            key: "changeme"

      - name: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        type: text-generation
        providers:
          - type: vllm
            model_name: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
            # Note: vLLM router service name follows pattern: <release-name>-router-service
            # Update this URL if you change the Helm release name
            url: "http://opengatellm-router-service/"
            key: "changeme"

      # If embeddings are deployed through vLLM, use this version :
#      - name: BAAI/bge-m3
#        type: text-embeddings-inference
#        providers:
#          - type: vllm
#            model_name: "BAAI/bge-m3"
#            # Note: vLLM router service name follows pattern: <release-name>-router-service
#            # Update this URL if you change the Helm release name
#            url: "http://opengatellm-router-service/"
#            key: "changeme"

      # If embeddings are deployed through TEI, use this version :
      - name: BAAI/bge-m3
        type: text-embeddings-inference
        dimensions: 1024
        providers:
          - type: tei
            model_name: "BAAI/bge-m3"
            # Note: TEI embeddings service name is defined in this stack chart
            # Pattern: opengatellm-stack-embeddings (hardcoded in templates/embeddings-service.yaml)
            url: "http://opengatellm-stack-embeddings/"
            key: "changeme"

    # -------------------------------- structured config override ---------------------------------
    structuredConfig:
      dependencies:
        postgres:
          url: ${POSTGRES_URI}
          echo: False
          pool_size: 5
          connect_args:
            server_settings:
              statement_timeout: "120s"
            command_timeout: 60
        redis:
          url: redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT}

        elasticsearch:
          # Service name follows pattern: <release-name>-elasticsearch-es-http (ECK pattern)
          hosts: "http://opengatellm-elasticsearch-es-http:9200"

      settings:
        log_level: INFO
        vector_store_model: "BAAI/bge-m3"

      playground:
        api_url: "http://opengatellm"
        default_model: ""
        app_title: "OpenGateLLM"
        theme_has_background: true
        theme_accent_color: "purple"
        theme_appearance: "light"
        theme_gray_color: "gray"
        theme_panel_background: "solid"
        theme_radius: "medium"
        theme_scaling: "100%"
        encryption_key: changeme
        session_secret_key: changeme
        proconnect_enabled: False
        postgres:
          url: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/playground

    probes:
      readiness:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 40
        periodSeconds: 15
      liveness:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 120
        periodSeconds: 30

playground:
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/playground
    tag: 0.3.6
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
    targetPort: 8501
  probes:
    readiness:
      path: /
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /
      initialDelaySeconds: 10
      periodSeconds: 10

vllm-stack:
  enabled: true
  routerSpec:
    serviceType: ClusterIP
    servicePort: 80
    resources:
      requests:
        cpu: 400m
        memory: 1Gi
      limits:
        memory: 2Gi
  servingEngineSpec:
    enableEngine: true
    vllmApiKey: "changeme"
    modelSpec:
      - name: "mistral-small"
        repository: vllm/vllm-openai
        tag: v0.11.0
        imagePullPolicy: IfNotPresent
        modelURL: mistralai/Mistral-Small-3.2-24B-Instruct-2506

        replicaCount: 1
        requestCPU: 20
        requestMemory: "200G"
        requestGPU: 1
        limitCPU: "22"
        limitMemory: "230G"

        pvcStorage: "230Gi"
        storageClass: bsu-gp2
        pvcAccessMode:
          - ReadWriteOnce

        shmSize: "32Gi"

        vllmConfig:
          tensorParallelSize: 1
          enablePrefixCaching: false
          extraArgs:
            - "--tokenizer_mode"
            - "mistral"
            - "--config_format"
            - "mistral"
            - "--load_format"
            - "mistral"
            - "--tool-call-parser"
            - "mistral"
            - "--enable-auto-tool-choice"

        hf_token:
          secretName: "huggingface-token"
          secretKey: "token"

# Embeddings through HF TEI
embeddings:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
    targetPort: 8000
  config:
    model: BAAI/bge-m3
    port: 8000
  resources:
    limits:
      cpu: "4"
      memory: 28Gi
      gpu: 0
    requests:
      cpu: "2"
      memory: 16Gi
      gpu: 0
  probes:
    readiness:
      enabled: true
      path: /health
      initialDelaySeconds: 30
      periodSeconds: 120
    liveness:
      enabled: false
      path: /health
      initialDelaySeconds: 240
      periodSeconds: 30

#      # Embeddings through VLLM - complicated to run on CPU (you have to build and publish the vllm image with CPU flag)
#      - name: "bge-embeddings"
#        repository: vllm/vllm-openai
#        tag: v0.11.0
#        imagePullPolicy: IfNotPresent
#
#        modelURL: BAAI/bge-m3
#        replicaCount: 1
#
#        # Override the default nvidia runtime - (we're running on CPU only)
#        runtimeClassName: ""
#
#        # Minimal resources (BGE-M3 is ~2GB, runs efficiently on CPU)
#        requestCPU: 2
#        requestMemory: "8Gi"
#        requestGPU: 0  # CPU only
#        limitCPU: "4"
#        limitMemory: "16Gi"
#
#        pvcStorage: "20Gi"
#        storageClass: sbs-default
#        pvcAccessMode:
#          - ReadWriteOnce
#
#        shmSize: "4Gi"
#
#        # Environment variables for debugging
#        env:
#          - name: VLLM_LOGGING_LEVEL
#            value: "DEBUG"
#
#        vllmConfig:
#          tensorParallelSize: 1
#          enablePrefixCaching: false
#          extraArgs:
#            - "--task"
#            - "embed"
#            - "--device"
#            - "cpu"
#
#        hf_token:
#         secretName: "huggingface-token"
#         secretKey: "token"

huggingface:
  token: "changeme"

# ES deployment follows this recommended deployment : https://www.elastic.co/docs/deploy-manage/deploy/cloud-on-k8s/install-using-helm-chart
elasticsearch:
  enabled: true
  version: "9.0.8"
  replicas: 1
  resources:
    requests:
      cpu: "100m"
      memory: "1Gi"
    limits:
      cpu: "400m"
      memory: "4Gi"
  persistence:
    enabled: true
    size: 8Gi

eck-operator-crds:
  {}

eck-operator:
  installCRDs: false
  createClusterScopedResources: true
  webhook:
    enabled: false
  config:
    validateStorageClass: false

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 150Mi
