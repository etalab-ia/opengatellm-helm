# Global settings
global:
  namespace: default
  storage:
    storageClassName: default

opengatellm:
  enabled: false
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/api
    tag: 0.2.4
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 80
    targetPort: 8000
  config:
    # ----------------------------------- models ------------------------------------
    models:
      - name: albert-testbed
        type: text-generation
        providers:
          - type: vllm
            model_name: "gemma3:1b"
            url: "http://albert-testbed.etalab.gouv.fr:8000"
            key: "changeme"

      - name: mistralai/Voxtral-Mini-3B-2507
        type: automatic-speech-recognition
        providers:
          - type: vllm
            model_name: "mistralai/Voxtral-Mini-3B-2507"
            url: "http://mistral-24b.default.svc.cluster.local/"
            key: "changeme"

      - name: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        type: automatic-speech-recognition
        providers:
          - type: vllm
            model_name: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
            url: "http://mistral-24b.default.svc.cluster.local/"
            key: "changeme"

      - name: embeddings
        type: text-embeddings-inference
        providers:
          - type: tei
            model_name: "BAAI/bge-m3"
            url: "http://bge-embeddings.default.svc.cluster.local/"
            key: "changeme"

    # -------------------------------- dependencies ---------------------------------
    dependencies:
      postgres:
        url: "postgresql+asyncpg://postgres:changeme@postgres:5432/api"
        echo: False
        pool_size: 5
        connect_args:
          server_settings:
            statement_timeout: "120s"
          command_timeout: 60

      redis:
        host: redis.default.svc.cluster.local
        port: 6379
        password: changeme

      # playground ---------------------------------------------------------------------------------------------------------------------------------------
    playground:
      api_url: "http://opengatellm.default.svc.cluster.local"
      encryption_key: changeme
      session_secret_key: changeme
      proconnect_enabled: False
      postgres:
        url: postgresql+asyncpg://postgres:changeme@postgres:5432/playground

  probes:
    readiness:
      enabled: false
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      enabled: false
      path: /health
      initialDelaySeconds: 60
      periodSeconds: 30

streamlit:
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/ui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 8501
    targetPort: 8501
  # TODO: should work without this config
  config:
    baseUrl: "http://opengatellm.default.svc.cluster.local:80"
    excludeModels: ""
    documentsEmbeddingsModel: "BAAI/bge-m3"
  probes:
    readiness:
      path: /
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /
      initialDelaySeconds: 10
      periodSeconds: 10

vllm:
  enabled: false
  replicas: 1
  image:
    repository: vllm/vllm-openai
    tag: v0.10.2
    # repository: ghcr.io/etalab-ia/albert-marker/server
    #tag: latest
    pullPolicy: IfNotPresent
  service:
    name: mistral-24b
    type: LoadBalancer
    port: 80
    targetPort: 8000
  config:
    # model: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
    # model: meta-llama/Llama-3.1-8B-Instruct
    # model: google/gemma-3-27b-it
    model: mistralai/Mistral-Small-3.2-24B-Instruct-2506
    # model: Qwen/Qwen3-30B-A3B-Instruct-2507
    # model: mistralai/Voxtral-Mini-3B-2507
    # model: mistralai/Magistral-Small-2506
    serveArgs: "--tokenizer_mode mistral --config_format mistral --load_format mistral --tensor-parallel-size 1 --tool-call-parser mistral --enable-auto-tool-choice --no-enable-prefix-caching"
    # serveArgs: "--no-enable-prefix-caching --max-model-len 63440 --tool-call-parser openai --enable-auto-tool-choice"
    # serveArgs: "--reasoning-parser qwen3 --enable-auto-tool-choice --tool-call-parser hermes --max-model-len 132896"

  persistence:
    size: 230Gi
    storageClassName: sbs-default
  resources:
#    limits:
#      cpu: "44"
#      memory: 460G
#      gpu: 2
#    requests:
#      cpu: "40"
#      memory: 400G
#      gpu: 2
    limits:
      cpu: "22"
      memory: 230G
      gpu: 1
    requests:
      cpu: "20"
      memory: 200G
      gpu: 1
  shm:
    sizeLimit: 32Gi
  secrets:
    huggingfaceToken:
      name: huggingface-token
      key: token
  probes:
    readiness:
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      path: /health
      initialDelaySeconds: 300
      periodSeconds: 30


whisperx:
  enabled: true
  replicas: 1
  image:
    repository: lasuite/whisper-api
    tag: whisperx-002
    pullPolicy: IfNotPresent
  service:
    name: whisper
    type: LoadBalancer
    port: 80
    targetPort: 8000
  persistence:
    size: 100Gi
    storageClassName: sbs-default
  resources:
    limits:
      cpu: "4"
      memory: 16Gi
      gpu: 1
    requests:
      cpu: "1"
      memory: 8G
      gpu: 1
  shm:
    sizeLimit: 2Gi
  secrets:
    apiKey:
      name: whisperx-api-key
      key: api_key
    huggingfaceToken:
      name: huggingface-token
      key: token
  probes:
    readiness:
      path: /health
      initialDelaySeconds: 30
      periodSeconds: 30
    liveness:
      path: /health
      initialDelaySeconds: 60
      periodSeconds: 30


# BGE Embeddings configuration
embeddings:
  enabled: false
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
    targetPort: 8000
  config:
    model: BAAI/bge-m3
    port: 8000
  resources:
    limits:
      cpu: "4"
      memory: 32Gi
      #gpu: 1
    requests:
      cpu: "2"
      memory: 16Gi
      #gpu: 1
  probes:
    readiness:
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      path: /health
      initialDelaySeconds: 60
      periodSeconds: 30

# Redis configuration
redis:
  replicas: 1
  image:
    repository: redis/redis-stack-server
    tag: 7.2.0-v11
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 6379
  config:
    args: "--dir /data --requirepass changeme --user username on >password ~* allcommands --save 60 1 --appendonly yes"
    password: "changeme"
    username: "username"
  persistence:
    size: 10Gi
  probes:
    readiness:
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      initialDelaySeconds: 60
      periodSeconds: 30

postgres:
  image:
    repository: postgres
    tag: "16.5"
    pullPolicy: IfNotPresent

  config:
    user: postgres
    password: changeme
    db: postgres
    create_db: api

  service:
    type: ClusterIP
    port: 5432

  persistence:
    enabled: true
    size: 8Gi
  scripts:
    enabled: true
    content: |
      #!/bin/bash
      
      function create_database() {
        local database=$1
        echo "  Creating database '$database'"
        psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" <<-EOSQL
            CREATE DATABASE $database WITH ENCODING 'utf8';
      EOSQL
      }
      
      if [[ -n "$CREATE_DB" ]]; then
        echo "Multiple database creation requested: $CREATE_DB"
        for db in $(echo $CREATE_DB | tr ',' ' '); do
         create_database $db
        done
        echo "Multiple databases created"
      fi

  healthcheck:
    enabled: true
    intervalSeconds: 4
    timeoutSeconds: 10
    failureThreshold: 5
    startupDelaySeconds: 60
