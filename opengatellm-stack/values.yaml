global:
  storage:
    storageClassName: default

opengatellm:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/api
    tag: 0.2.7
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 80
    targetPort: 8000
  config:
    # ----------------------------------- models ------------------------------------
    models:
      - name: albert-testbed
        type: text-generation
        providers:
          - type: vllm
            model_name: "gemma3:1b"
            url: "http://albert-testbed.etalab.gouv.fr:8000"
            key: "changeme"

      - name: mistralai/Voxtral-Mini-3B-2507
        type: automatic-speech-recognition
        providers:
          - type: vllm
            model_name: "mistralai/Voxtral-Mini-3B-2507"
            # Note: Update service name based on your release name: <release-name>-router-service
            url: "http://opengatellm-stack-router-service/"
            key: "changeme"

      - name: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        type: text-generation
        providers:
          - type: vllm
            model_name: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
            # Note: Update service name based on your release name: <release-name>-router-service
            url: "http://opengatellm-stack-router-service/"
            key: "changeme"

      - name: BAAI/bge-m3
        type: text-embeddings-inference
        providers:
          - type: tei
            model_name: "BAAI/bge-m3"
            url: "http://bge-embeddings.opengatellm.svc.cluster.local/"
            key: "changeme"

    # -------------------------------- dependencies ---------------------------------
    dependencies:
      postgres:
        # Note: Update service name based on your release name: <release-name>-postgresql
        url: "postgresql+asyncpg://postgres:changeme@opengatellm-stack-postgresql:5432/api"
        echo: False
        pool_size: 5
        connect_args:
          server_settings:
            statement_timeout: "120s"
          command_timeout: 60

      redis:
        # Note: Update service name based on your release name: <release-name>-redis-master
        host: opengatellm-stack-redis-master
        port: 6379
        password: changeme

      elasticsearch:
        # Note: Service name matches elasticsearch.service.name in values
        hosts: "http://opengatellm-stack-elasticsearch:9200"
        # No authentication needed (security disabled for simplicity)
        # basic_auth:
        #   - "elastic"
        #   - "changeme"

      # playground ---------------------------------------------------------------------------------------------------------------------------------------

    settings:
      vector_store_model: "BAAI/bge-m3"

    playground:
      api_url: "http://opengatellm.opengatellm.svc.cluster.local"
      encryption_key: changeme
      session_secret_key: changeme
      proconnect_enabled: False
      postgres:
        # Note: Update service name based on your release name: <release-name>-postgresql
        url: postgresql+asyncpg://postgres:changeme@opengatellm-stack-postgresql:5432/playground

  probes:
    readiness:
      enabled: false
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      enabled: false
      path: /health
      initialDelaySeconds: 60
      periodSeconds: 30

streamlit:
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/playground
    tag: 0.2.7
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 8501
    targetPort: 8501
  probes:
    readiness:
      path: /
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /
      initialDelaySeconds: 10
      periodSeconds: 10

vllm-stack:
  enabled: false
  # Expose vLLM router service externally
  serviceType: LoadBalancer
  servicePort: 80
  # Router resource configuration (default 500Mi is too small, causing OOMKills)
  routerSpec:
    resources:
      requests:
        cpu: 400m
        memory: 1Gi
      limits:
        memory: 2Gi
  servingEngineSpec:
    enableEngine: true
    # API key for securing vLLM - value will be overridden from values-secrets.yaml
    vllmApiKey: "changeme"
    modelSpec:
      - name: "mistral-small"
        repository: vllm/vllm-openai
        tag: v0.11.0
        imagePullPolicy: IfNotPresent

        # Model configuration
        modelURL: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        # modelURL: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
        # modelURL: meta-llama/Llama-3.1-8B-Instruct
        # modelURL: google/gemma-3-27b-it
        # modelURL: Qwen/Qwen3-30B-A3B-Instruct-2507
        # modelURL: mistralai/Voxtral-Mini-3B-2507
        # modelURL: mistralai/Magistral-Small-2506

        replicaCount: 1

        # Resource configuration
        requestCPU: 20
        requestMemory: "200G"
        requestGPU: 1
        limitCPU: "22"
        limitMemory: "230G"

        pvcStorage: "230Gi"
        storageClass: sbs-default
        pvcAccessMode:
          - ReadWriteOnce

        shmSize: "32Gi"

        vllmConfig:
          tensorParallelSize: 1
          enablePrefixCaching: false
          extraArgs:
            - "--tokenizer_mode"
            - "mistral"
            - "--config_format"
            - "mistral"
            - "--load_format"
            - "mistral"
            - "--tool-call-parser"
            - "mistral"
            - "--enable-auto-tool-choice"

        # Hugging Face token from existing secret
        hf_token:
          secretName: "huggingface-token"
          secretKey: "token"

# BGE Embeddings configuration
embeddings:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
    targetPort: 8000
  config:
    model: BAAI/bge-m3
    port: 8000
  resources:
    limits:
      cpu: "4"
      memory: 32Gi
      #gpu: 1
    requests:
      cpu: "2"
      memory: 16Gi
      #gpu: 1
  probes:
    readiness:
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      path: /health
      initialDelaySeconds: 120
      periodSeconds: 30

redis:
  enabled: true
  fullnameOverride: opengatellm-stack-redis
  architecture: standalone
  auth:
    enabled: true
    password: "changeme"
  image:
    registry: docker.io
    repository: redis/redis-stack-server
    tag: 7.4.0-v8
  global:
    security:
      # Allow official Redis Stack image instead of the default minimal bitnami one
      allowInsecureImages: true
  commonConfiguration: |-
    # Load Redis Stack modules from correct path
    loadmodule /opt/redis-stack/lib/redistimeseries.so
    loadmodule /opt/redis-stack/lib/rejson.so
    loadmodule /opt/redis-stack/lib/redisearch.so
    loadmodule /opt/redis-stack/lib/redisbloom.so
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master:
    persistence:
      enabled: true
      size: 10Gi
    resources: {}
  replica:
    # Single instance mode
    replicaCount: 0


postgresql:
  enabled: true
  auth:
    username: postgres
    password: changeme
    database: postgres
  primary:
    persistence:
      enabled: true
      size: 8Gi
    initdb:
      scripts:
        init-db.sql: |
          CREATE DATABASE api WITH ENCODING 'utf8';
    resources: {}

elasticsearch:
  enabled: true
  # Single node configuration
  replicas: 1
  minimumMasterNodes: 1

  # Cluster name
  clusterName: "opengatellm-elasticsearch"

  # Resources
  resources:
    requests:
      cpu: "100m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  # Persistence
  persistence:
    enabled: true
    size: 8Gi

  # Disable security for simplicity (no TLS, no auth)
  esConfig:
    elasticsearch.yml: |
      xpack.security.enabled: false
      xpack.security.transport.ssl.enabled: false
      xpack.security.http.ssl.enabled: false

  # Service
  service:
    type: ClusterIP
    name: opengatellm-stack-elasticsearch