global:
  storage:
    storageClassName: default

opengatellm:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/api
    tag: 0.3.0
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 80
    targetPort: 8000
  config:
    # ----------------------------------- models ------------------------------------
    models:
      - name: albert-testbed
        type: text-generation
        providers:
          - type: vllm
            model_name: "gemma3:1b"
            url: "http://albert-testbed.etalab.gouv.fr:8000"
            key: "changeme"

      - name: mistralai/Voxtral-Mini-3B-2507
        type: automatic-speech-recognition
        providers:
          - type: vllm
            model_name: "mistralai/Voxtral-Mini-3B-2507"
            # Note: Update service name based on your release name: <release-name>-router-service
            url: "http://opengatellm-stack-router-service/"
            key: "changeme"

      - name: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        type: text-generation
        providers:
          - type: vllm
            model_name: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
            # Note: Update service name based on your release name: <release-name>-router-service
            url: "http://opengatellm-stack-router-service/"
            key: "changeme"

      # If embeddings are deployed through vLLM, use this version :
#      - name: BAAI/bge-m3
#        type: text-embeddings-inference
#        providers:
#          - type: vllm
#            model_name: "BAAI/bge-m3"
#            url: "http://opengatellm-stack-router-service/"
#            key: "changeme"

        # If embeddings are deployed through TEI, use this version :
      - name: BAAI/bge-m3
        type: text-embeddings-inference
        dimensions: 1024
        providers:
         - type: tei
           model_name: "BAAI/bge-m3"
           url: "http://opengatellm-stack-embeddings/"
           key: "changeme"

    # -------------------------------- dependencies ---------------------------------
    dependencies:
      postgres:
        # Note: Update service name based on your release name: <release-name>-postgresql
        url: "postgresql+asyncpg://postgres:changeme@opengatellm-stack-postgresql:5432/api"
        echo: False
        pool_size: 5
        connect_args:
          server_settings:
            statement_timeout: "120s"
          command_timeout: 60
      redis:
        # Note: Update service name based on your release name: <release-name>-redis-master
        host: opengatellm-stack-redis-master
        port: 6379
        password: changeme

      elasticsearch:
        # Note: Service name from Elastic chart is <release>-elasticsearch-master
        hosts: "http://opengatellm-elasticsearch-master:9200"

    settings:
      vector_store_model: "BAAI/bge-m3"

    playground:
      # Playground configuration (used by 0.3.0+)
      api_url: "http://opengatellm-stack-api"
      default_model: ""  # Optional: first model selected in chat page
      app_title: "OpenGateLLM"
      # Theme configuration
      theme_has_background: true
      theme_accent_color: "purple"
      theme_appearance: "light"
      theme_gray_color: "gray"
      theme_panel_background: "solid"
      theme_radius: "medium"
      theme_scaling: "100%"
      # Legacy settings (kept for backward compatibility if needed)
      encryption_key: changeme
      session_secret_key: changeme
      proconnect_enabled: False
      postgres:
        # Note: Update service name based on your release name: <release-name>-postgresql
        url: postgresql+asyncpg://postgres:changeme@opengatellm-stack-postgresql:5432/playground

  probes:
    readiness:
      enabled: false
      path: /health
      initialDelaySeconds: 10
      periodSeconds: 30
    liveness:
      enabled: false
      path: /health
      initialDelaySeconds: 60
      periodSeconds: 30

playground:
  replicas: 1
  image:
    repository: ghcr.io/etalab-ia/opengatellm/playground
    tag: 0.3.0
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 80
    targetPort: 80
  probes:
    readiness:
      path: /
      initialDelaySeconds: 5
      periodSeconds: 10
    liveness:
      path: /
      initialDelaySeconds: 10
      periodSeconds: 10

vllm-stack:
  enabled: true
  serviceType: LoadBalancer
  servicePort: 80
  # Router resource configuration (default 500Mi is too small, causing OOMKills)
  routerSpec:
    resources:
      requests:
        cpu: 400m
        memory: 1Gi
      limits:
        memory: 2Gi
  servingEngineSpec:
    enableEngine: true
    # API key for securing vLLM - value will be overridden from values-secrets.yaml
    vllmApiKey: "changeme"
    modelSpec:
      - name: "mistral-small"
        repository: vllm/vllm-openai
        tag: v0.11.0
        imagePullPolicy: IfNotPresent

        # Model configuration
        modelURL: mistralai/Mistral-Small-3.2-24B-Instruct-2506
        # modelURL: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
        # modelURL: meta-llama/Llama-3.1-8B-Instruct
        # modelURL: google/gemma-3-27b-it
        # modelURL: Qwen/Qwen3-30B-A3B-Instruct-2507
        # modelURL: mistralai/Voxtral-Mini-3B-2507
        # modelURL: mistralai/Magistral-Small-2506

        replicaCount: 1

        # Resource configuration
        requestCPU: 20
        requestMemory: "200G"
        requestGPU: 1
        limitCPU: "22"
        limitMemory: "230G"

        pvcStorage: "230Gi"
        storageClass: sbs-default
        pvcAccessMode:
          - ReadWriteOnce

        shmSize: "32Gi"

        vllmConfig:
          tensorParallelSize: 1
          enablePrefixCaching: false
          extraArgs:
            - "--tokenizer_mode"
            - "mistral"
            - "--config_format"
            - "mistral"
            - "--load_format"
            - "mistral"
            - "--tool-call-parser"
            - "mistral"
            - "--enable-auto-tool-choice"

        # Hugging Face token from existing secret
        hf_token:
          secretName: "huggingface-token"
          secretKey: "token"

# Embeddings through HF TEI
embeddings:
  enabled: true
  replicas: 1
  image:
    repository: ghcr.io/huggingface/text-embeddings-inference
    tag: cpu-latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
    targetPort: 8000
  config:
    model: BAAI/bge-m3
    port: 8000
  resources:
    limits:
      cpu: "4"
      memory: 28Gi
      gpu: 0
    requests:
      cpu: "2"
      memory: 16Gi
      gpu: 0
  probes:
    readiness:
      enabled: true
      path: /health
      initialDelaySeconds: 30
      periodSeconds: 120
    liveness:
      enabled: false
      path: /health
      initialDelaySeconds: 240
      periodSeconds: 30

#      # Embeddings through VLLM - complicated to run on CPU (you have to build and publish the vllm image with CPU flag)
#      - name: "bge-embeddings"
#        repository: vllm/vllm-openai
#        tag: v0.11.0
#        imagePullPolicy: IfNotPresent
#
#        modelURL: BAAI/bge-m3
#        replicaCount: 1
#
#        # Override the default nvidia runtime - (we're running on CPU only)
#        runtimeClassName: ""
#
#        # Minimal resources (BGE-M3 is ~2GB, runs efficiently on CPU)
#        requestCPU: 2
#        requestMemory: "8Gi"
#        requestGPU: 0  # CPU only
#        limitCPU: "4"
#        limitMemory: "16Gi"
#
#        pvcStorage: "20Gi"
#        storageClass: sbs-default
#        pvcAccessMode:
#          - ReadWriteOnce
#
#        shmSize: "4Gi"
#
#        # Environment variables for debugging
#        env:
#          - name: VLLM_LOGGING_LEVEL
#            value: "DEBUG"
#
#        vllmConfig:
#          tensorParallelSize: 1
#          enablePrefixCaching: false
#          extraArgs:
#            - "--task"
#            - "embed"
#            - "--device"
#            - "cpu"
#
#        hf_token:
#         secretName: "huggingface-token"
#         secretKey: "token"

redis:
  enabled: true
  fullnameOverride: opengatellm-stack-redis
  architecture: standalone
  auth:
    enabled: true
    password: "changeme"
  image:
    registry: docker.io
    repository: redis/redis-stack-server
    tag: 7.4.0-v8
  global:
    security:
      # Allow official Redis Stack image instead of the default minimal bitnami one
      allowInsecureImages: true
  commonConfiguration: |-
    # Load Redis Stack modules from correct path
    loadmodule /opt/redis-stack/lib/redistimeseries.so
    loadmodule /opt/redis-stack/lib/rejson.so
    loadmodule /opt/redis-stack/lib/redisearch.so
    loadmodule /opt/redis-stack/lib/redisbloom.so
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master:
    persistence:
      enabled: true
      size: 10Gi
    resources: {}
  replica:
    # Single instance mode
    replicaCount: 0


postgresql:
  enabled: true
  auth:
    username: postgres
    password: changeme
    database: postgres
  primary:
    persistence:
      enabled: true
      size: 8Gi
    initdb:
      scripts:
        init-db.sql: |
          CREATE DATABASE api WITH ENCODING 'utf8';
    resources: {}

elasticsearch:
  enabled: true
  # Single node configuration
  replicas: 1
  minimumMasterNodes: 1

  # Cluster name
  clusterName: "opengatellm-elasticsearch"

  # Resources
  resources:
    requests:
      cpu: "100m"
      memory: "1Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  # Persistence
  persistence:
    enabled: true
    size: 8Gi

  # Disable security for simplicity (no TLS, no auth)
  esConfig:
    elasticsearch.yml: |
      xpack.security.enabled: false
      xpack.security.transport.ssl.enabled: false
      xpack.security.http.ssl.enabled: false

  # Service (actual name will be: <release>-elasticsearch-master)
  service:
    type: ClusterIP